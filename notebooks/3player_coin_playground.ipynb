{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Set Up the Environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12c3ee0300b16a3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import flax\n",
    "import hydra # it is just there to read the config files for us, hydra has some logic, please read its tutorial\n",
    "from omegaconf import DictConfig, OmegaConf # essentially hydra things\n",
    "import jax.numpy as jp \n",
    "import jax.random as rax\n",
    "from jax import config # in order to be able to disable jit so we can debug, or to debug nans (not always on for performance slowdown it causes)\n",
    "# Update JAX configuration\n",
    "config.update('jax_disable_jit', True)\n",
    "config.update(\"jax_debug_nans\", True)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-13T09:17:14.930734Z",
     "start_time": "2024-02-13T09:17:14.814097Z"
    }
   },
   "id": "initial_id",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the hp with hydra"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c5d496d9d090630"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: FrozenDict({\n",
      "    seed: 45,\n",
      "    reward_discount: 0.96,\n",
      "    batch_size: 128,\n",
      "    agent_0: 'loqa',\n",
      "    agent_1: 'loqa',\n",
      "    eval_every: 100,\n",
      "    op_softmax_temp: 1.0,\n",
      "    game: {\n",
      "        height: 3,\n",
      "        width: 3,\n",
      "        gnumactions: 4,\n",
      "        game_length: 50,\n",
      "    },\n",
      "    save_dir: './experiments',\n",
      "    save_every: 100,\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Manually loading the configuration\n",
    "config_path = \"../conf/coin_conf/coin_config.yaml\"  # Adjust path as necessary\n",
    "cfg = OmegaConf.load(config_path)\n",
    "# Work with the configuration\n",
    "hp = OmegaConf.to_container(cfg.hp, resolve=True)  # Converts cfg to a Python dict\n",
    "hp = flax.core.FrozenDict(hp)\n",
    "print(f'Hyperparameters: {hp}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T09:17:15.757951Z",
     "start_time": "2024-02-13T09:17:15.741274Z"
    }
   },
   "id": "9448521be63a4c6a",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization and setting up the environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c2a1a70e0df1ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from three_player_coin_game import ThreePlayerCoinGame\n",
    "\n",
    "assert hp['just_self_play'] == False, 'This notebook does not support self play, because it is complex, and this needs to be a simple example'\n",
    "just_self_play = False\n",
    "\n",
    "dummy_env, _ = ThreePlayerCoinGame.init(\n",
    "    rng=rax.PRNGKey(hp['seed']),\n",
    "    **coin_game_params(hp),\n",
    ")\n",
    "dummy_episode = make_zero_episode(trace_length=hp['game']['game_length'], coin_game=dummy_env)\n",
    "\n",
    "\n",
    "state = dict()\n",
    "state['rng'] = rax.PRNGKey(hp['seed'])\n",
    "agent_module = GRUCoinAgent(hidden_size_actor=hp['actor']['hidden_size'],\n",
    "                            hidden_size_qvalue=hp['qvalue']['hidden_size'],\n",
    "                            layers_before_gru_actor=hp['actor']['layers_before_gru'],\n",
    "                            layers_before_gru_qvalue=hp['qvalue']['layers_before_gru'], )\n",
    "dummy_rng = rax.PRNGKey(0)\n",
    "rng, rng1, rng2 = rax.split(state['rng'], 3)\n",
    "state['step'] = 0\n",
    "state['rng'] = rng\n",
    "dummy_obs_seq = dummy_episode['obs'][:, 0].reshape(dummy_episode['obs'].shape[0], -1)\n",
    "\n",
    "def set_up_agent_nn(player_id):\n",
    "    agent_params = agent_module.init(rng1, {'obs_seq': dummy_obs_seq, 'rng': dummy_rng, 't': 0})\n",
    "    agent = CoinAgent(params=agent_params, model=agent_module, player=player_id)\n",
    "    state[f'agent{player_id}'] = agent\n",
    "\n",
    "set_up_agent_nn(player_id=0)\n",
    "agent0 = state['agent0']\n",
    "if not just_self_play:\n",
    "    set_up_agent_nn(player_id=1)\n",
    "    agent1 = state['agent1']\n",
    "    set_up_agent_nn(player_id=2)\n",
    "    agent2 = state['agent2']\n",
    "\n",
    "# --- defining replay buffers ---\n",
    "def create_rb_agent_params(player_id: int):\n",
    "    rb_size = hp['agent_replay_buffer']['capacity']\n",
    "    tmp_rb = [state[f'agent{player_id}'].params for _ in range(rb_size)]\n",
    "    state[f'rb_agent{player_id}_params'] = jax.tree_map(lambda *xs: jp.stack(xs, axis=0), *tmp_rb)\n",
    "    state['min_valid_index_rb'] = rb_size  # first, the buffer is not valid\n",
    "\n",
    "if use_rb(hp):\n",
    "    create_rb_agent_params(player_id=0)\n",
    "    if not just_self_play:\n",
    "        create_rb_agent_params(player_id=1)\n",
    "        create_rb_agent_params(player_id=2)\n",
    "\n",
    "# --- defining ema ---\n",
    "state['agent0_ema'] = agent0\n",
    "if not just_self_play:\n",
    "    state['agent1_ema'] = agent1\n",
    "    state['agent2_ema'] = agent2\n",
    "\n",
    "# --- defining optimizers ---\n",
    "if hp['actor']['train']['optimizer'] == 'adam':\n",
    "    actor_opt_module = optax.adam\n",
    "elif hp['actor']['train']['optimizer'] == 'sgd':\n",
    "    actor_opt_module = optax.sgd\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimizer: {hp['actor']['train']['optimizer']}\")\n",
    "\n",
    "actor_train_separate = hp['actor']['train']['separate']\n",
    "if actor_train_separate == 'enabled':\n",
    "    actor_agent_lr = hp['actor']['train']['lr_loss_actor_agent']\n",
    "    actor_opponent_lr = hp['actor']['train']['lr_loss_actor_opponent']\n",
    "    actor_opt_agent = actor_opt_module(learning_rate=actor_agent_lr)\n",
    "    actor_opt_opponent = actor_opt_module(learning_rate=actor_opponent_lr)\n",
    "\n",
    "    def setup_actor_optimizer(player_id):\n",
    "        agent = state[f'agent{player_id}']\n",
    "        state[f'agent{player_id}_opt_actor_loss_agent'] = Optimizer(actor_opt_agent, actor_opt_agent.init(agent))\n",
    "        state[f'agent{player_id}_opt_actor_loss_opponent'] = Optimizer(actor_opt_opponent, actor_opt_opponent.init(agent))\n",
    "\n",
    "elif actor_train_separate == 'disabled':\n",
    "    lr = hp['actor']['train']['lr_loss_actor']\n",
    "    actor_opt = actor_opt_module(learning_rate=lr)\n",
    "\n",
    "    def setup_actor_optimizer(player_id):\n",
    "        agent = state[f'agent{player_id}']\n",
    "        state[f'agent{player_id}_opt_actor_loss'] = Optimizer(actor_opt, actor_opt.init(agent))\n",
    "else:\n",
    "    raise ValueError(f\"Unknown separate: {hp['actor']['train']['separate']}\")\n",
    "\n",
    "setup_actor_optimizer(player_id=0)\n",
    "if not just_self_play:\n",
    "    setup_actor_optimizer(player_id=1)\n",
    "    setup_actor_optimizer(player_id=2)\n",
    "\n",
    "critic_lr = hp['qvalue']['train']['lr_loss_qvalue']\n",
    "if hp['qvalue']['train']['optimizer'] == 'adam':\n",
    "    qvalue_opt = optax.adam(learning_rate=critic_lr)\n",
    "elif hp['qvalue']['train']['optimizer'] == 'sgd':\n",
    "    qvalue_opt = optax.sgd(learning_rate=critic_lr)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown optimizer: {hp['qvalue']['train']['optimizer']}\")\n",
    "\n",
    "if hp['qvalue']['replay_buffer']['mode'] == 'disabled':\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(f'Unknown replay buffer mode: {hp[\"qvalue\"][\"replay_buffer\"][\"mode\"]}')\n",
    "\n",
    "state['agent0_opt_qvalue'] = Optimizer(qvalue_opt, qvalue_opt.init(agent0))\n",
    "if not just_self_play:\n",
    "    state['agent1_opt_qvalue'] = Optimizer(qvalue_opt, qvalue_opt.init(agent1))\n",
    "    state['agent2_opt_qvalue'] = Optimizer(qvalue_opt, qvalue_opt.init(agent2))\n",
    "\n",
    "c_0 = agent0.get_initial_carries()\n",
    "c_0_actor = c_0['carry_actor']\n",
    "c_0_qvalue = c_0['carry_qvalue']\n",
    "\n",
    "if not just_self_play:\n",
    "    c_1 = agent1.get_initial_carries()\n",
    "    c_1_actor = c_1['carry_actor']\n",
    "    c_1_qvalue = c_1['carry_qvalue']\n",
    "    c_2 = agent2.get_initial_carries()\n",
    "    c_2_actor = c_2['carry_actor']\n",
    "    c_2_qvalue = c_2['carry_qvalue']\n",
    "else:\n",
    "    c_1_actor = c_0_actor\n",
    "    c_1_qvalue = c_0_qvalue\n",
    "    c_2_actor = c_0_actor\n",
    "    c_2_qvalue = c_0_qvalue\n",
    "\n",
    "carries = {'c_0_actor': c_0_actor,\n",
    "           'c_0_qvalue': c_0_qvalue,\n",
    "           'c_1_actor': c_1_actor,\n",
    "           'c_1_qvalue': c_1_qvalue,\n",
    "           'c_2_actor': c_2_actor,\n",
    "           'c_2_qvalue': c_2_qvalue,\n",
    "           }\n",
    "\n",
    "return state, carries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93dce6da5342f078"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
